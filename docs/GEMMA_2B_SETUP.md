# üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Gemma 2b

**–î–æ–∫—É–º–µ–Ω—Ç**: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é Gemma 2b —Å Arvis  
**–î–∞—Ç–∞**: October 21, 2025  
**–°—Ç–∞—Ç—É—Å**: –ì–æ—Ç–æ–≤–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è  
**–Ø–∑—ã–∫**: –†—É—Å—Å–∫–∏–π (Russian) + English

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–ß—Ç–æ —Ç–∞–∫–æ–µ Gemma 2b?](#—á—Ç–æ-—Ç–∞–∫–æ–µ-gemma-2b)
2. [–°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è](#—Å–∏—Å—Ç–µ–º–Ω—ã–µ-—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
3. [–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ Ollama](#—É—Å—Ç–∞–Ω–æ–≤–∫–∞-—á–µ—Ä–µ–∑-ollama)
4. [–ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è](#–ø—Ä—è–º–∞—è-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
5. [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤ Arvis](#–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è-–≤-arvis)
6. [–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ](#–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
7. [–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è](#–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è)
8. [Troubleshooting](#troubleshooting)

---

## –ß—Ç–æ —Ç–∞–∫–æ–µ Gemma 2b?

**Gemma 2b** ‚Äî —ç—Ç–æ –ª–µ–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google:

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ |
|----------|----------|
| **–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫** | Google |
| **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã** | 2 –º–ª—Ä–¥ (2B) |
| **–†–∞–∑–º–µ—Ä** | ~5 –ì–ë –Ω–∞ –¥–∏—Å–∫–µ |
| **RAM** | 4-6 –ì–ë (–º–∏–Ω–∏–º—É–º) |
| **–°–∫–æ—Ä–æ—Å—Ç—å** | –ë—ã—Å—Ç—Ä–∞—è (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–ª—è –±–æ–µ–≤—ã—Ö —Å–∏—Å—Ç–µ–º) |
| **–ö–∞—á–µ—Å—Ç–≤–æ** | –•–æ—Ä–æ—à–µ–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ |
| **–õ–∏—Ü–µ–Ω–∑–∏—è** | Apache 2.0 |

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞**:
- ‚úÖ –õ–µ–≥—á–µ, —á–µ–º Mistral (7B) –∏–ª–∏ Llama 2 (7B)
- ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ, —á–µ–º –±–æ–ª—å—à–∏–µ –º–æ–¥–µ–ª–∏
- ‚úÖ –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞/—Å–∫–æ—Ä–æ—Å—Ç–∏
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
- ‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–µ–≤—ã—Ö —Å–∏—Å—Ç–µ–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç—å—é

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏**:
- ‚ö†Ô∏è –ú–µ–Ω—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (4K —Ç–æ–∫–µ–Ω–æ–≤)
- ‚ö†Ô∏è –ú–µ–Ω–µ–µ "–∑–Ω–∞—é—â–∞—è" —á–µ–º GPT
- ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç Ollama –∏–ª–∏ –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

---

## –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ú–∏–Ω–∏–º—É–º (–Ω–∞ –ø—Ä–µ–¥–µ–ª–µ)
```
CPU: Intel Core i5 (6-—è–¥–µ—Ä–Ω—ã–π) / AMD Ryzen 5
RAM: 6 –ì–ë (+ 4 –ì–ë –ø–æ–¥–∫–∞—á–∫–∞ –Ω–∞ –¥–∏—Å–∫–µ)
GPU: –ù–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è, –Ω–æ —É—Å–∫–æ—Ä–∏—Ç —Ä–∞–±–æ—Ç—É (NVIDIA, AMD)
SSD: 10 –ì–ë —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞
```

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
```
CPU: Intel Core i7 / AMD Ryzen 7 (8+ —è–¥–µ—Ä)
RAM: 8-16 –ì–ë
GPU: NVIDIA (CUDA) –∏–ª–∏ AMD (ROCm) - —Å–∏–ª—å–Ω–æ —É—Å–∫–æ—Ä—è–µ—Ç
SSD: 20 –ì–ë —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞
```

### –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É —Å–µ–±—è

**Windows PowerShell:**
```powershell
# –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä
Get-ComputerInfo | Select-Object CsProcessors

# –ü–∞–º—è—Ç—å
Get-ComputerInfo | Select-Object CsTotalPhysicalMemory

# –î–∏—Å–∫ (C:)
Get-Volume -DriveLetter C | Select-Object SizeRemaining
```

---

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ Ollama

### –í–∞—Ä–∏–∞–Ω—Ç 1: –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)

#### –®–∞–≥ 1: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
1. –°–∫–∞—á–∞—Ç—å [ollama.ai](https://ollama.ai) –¥–ª—è Windows/Mac/Linux
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—ã—á–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
3. –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä

#### –®–∞–≥ 2: –ó–∞–≥—Ä—É–∑–∏—Ç—å Gemma 2b
```bash
# –û—Ç–∫—Ä–æ–π—Ç–µ PowerShell/Terminal –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ

ollama pull gemma:2b

# –í—ã–≤–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å:
# pulling manifest ‚úì
# pulling 5f9b... ‚úì
# Success! Model downloaded
```

‚è±Ô∏è **–í—Ä–µ–º—è –ø–µ—Ä–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏**: 5-15 –º–∏–Ω—É—Ç (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)

#### –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –æ–¥–∏–Ω —Ä–∞–∑
ollama run gemma:2b

# –í–≤–µ—Å—Ç–∏: "Hello"
# –í—ã—Ö–æ–¥: Gemma –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—Ç–∏—Ç—å

# –í—ã—Ö–æ–¥: Ctrl+D –∏–ª–∏ quit
```

‚úÖ –ì–æ—Ç–æ–≤–æ! Gemma 2b —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç.

---

### –í–∞—Ä–∏–∞–Ω—Ç 2: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å—Ä–∞–∑—É

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):

```bash
# Gemma 2b (–±—ã—Å—Ç—Ä–∞—è)
ollama pull gemma:2b

# Mistral 7b (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ, –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
ollama pull mistral:7b

# Phi 2b (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Gemma)
ollama pull phi:2b

# –ó–µ–π—Ñ—É—Ä (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–∞—è)
ollama pull zephyr:7b
```

---

## –ü—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è

### –ï—Å–ª–∏ Ollama –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç

–ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Gemma 2b –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ Python:

#### –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
```bash
# Windows PowerShell
.venv\Scripts\pip install transformers torch bitsandbytes

# –ï—Å–ª–∏ –Ω–∞ GPU (NVIDIA)
.venv\Scripts\pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# –ü–µ—Ä–≤—ã–π —Ä–∞–∑ –∑–∞–≥—Ä—É–∑–∏—Ç—Å—è ~5 –ì–ë —Å Hugging Face
model_id = "google/gemma-2b-it"  # 'it' = Instruct-tuned (–ª—É—á—à–µ –¥–ª—è —á–∞—Ç–∞)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,  # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
    device_map="auto"            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞ GPU –µ—Å–ª–∏ –µ—Å—Ç—å
)
```

#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
```python
prompt = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

---

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤ Arvis

### –û–±–Ω–æ–≤–∏—Ç—å config.json

```json
{
    "llm": {
        "default_model": "gemma:2b",
        "ollama_url": "http://localhost:11434",
        "temperature": 0.7,
        "max_tokens": 512,
        "stream": true,
        "models": {
            "gemma:2b": {
                "name": "Gemma 2B",
                "description": "–ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–æ–µ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è",
                "context_length": 8192,
                "parameters": 2000000000,
                "recommend_for": ["fast", "low_memory", "russian"]
            },
            "mistral:7b": {
                "name": "Mistral 7B",
                "description": "–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ",
                "context_length": 32000,
                "parameters": 7000000000,
                "recommend_for": ["quality", "long_context"]
            }
        }
    }
}
```

### –ò–ª–∏ –≤ code (Python)

```python
from config.config import Config

config = Config()
config.set("llm.default_model", "gemma:2b")
config.set("llm.temperature", 0.7)
config.set("llm.max_tokens", 512)  # –ú–µ–Ω—å—à–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ—Å—Ç–∏
```

---

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
from utils.operation_mode_manager import OperationModeManager
from utils.providers.llm.gemma_provider import GemmaLLMProvider

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
manager = OperationModeManager(config)
gemma_provider = GemmaLLMProvider(config)
manager.register_provider(gemma_provider)

# –ó–∞–ø—É—Å—Ç–∏—Ç—å
if manager.initialize_mode():
    # –ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
    response = gemma_provider.generate_response(
        prompt="–ü—Ä–∏–≤–µ—Ç, Gemma! –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Å—è.",
        temperature=0.7,
        max_tokens=256
    )
    print(response)
    
    # –ü–æ—Ç–æ–∫–æ–º (streaming)
    for chunk in gemma_provider.stream_response(
        prompt="–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ 3 –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Ñ–∞–∫—Ç–∞ –æ Python",
        max_tokens=512
    ):
        print(chunk, end="", flush=True)
```

### –° —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º

```python
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–æ–ª—å –º–æ–¥–µ–ª–∏
response = gemma_provider.generate_response(
    prompt="–ö–∞–∫–æ–π —Å–µ–π—á–∞—Å –≥–æ–¥?",
    system_prompt="–¢—ã - –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ.",
    temperature=0.5  # –ù–∏–∂–µ ‚Üí –±–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
)
```

### –° fallback –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—Å—è –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ Gemma –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
result = manager.llm_fallback.execute(
    operation=lambda p: p.generate_response("–ü—Ä–∏–≤–µ—Ç!"),
    operation_name="llm_generation"
)
```

---

## –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

### 1. –£–º–µ–Ω—å—à–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è –±–æ–µ–≤–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

```json
{
    "llm": {
        "temperature": 0.3,  // –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
        "max_tokens": 256    // –ö–æ—Ä–æ—á–µ, –±—ã—Å—Ç—Ä–µ–µ
    }
}
```

### 2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å quantization (—Å–∂–∞—Ç–∏–µ)

**–≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å –Ω–∞ 50%**, –Ω–æ –Ω–µ–º–Ω–æ–≥–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ:

```bash
ollama pull gemma:2b-q4_K_M  # Quantized –≤–µ—Ä—Å–∏—è
```

### 3. –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç

```python
response = gemma_provider.generate_response(
    prompt=prompt,
    max_tokens=256,  # –í–º–µ—Å—Ç–æ 512
    temperature=0.5
)
```

### 4. –ö–µ—à–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_cached_response(prompt: str) -> str:
    return gemma_provider.generate_response(prompt)
```

### 5. –ó–∞–ø—É—Å–∫–∞—Ç—å Ollama –≤ —Ñ–æ–Ω–µ

```json
{
    "startup": {
        "autostart_ollama": true,
        "ollama_launch_mode": "background"
    }
}
```

---

## Troubleshooting

### ‚ùå "Ollama is not reachable"

**–†–µ—à–µ–Ω–∏–µ 1**: –ó–∞–ø—É—Å—Ç–∏—Ç—å Ollama
```bash
ollama serve
```

**–†–µ—à–µ–Ω–∏–µ 2**: –ü—Ä–æ–≤–µ—Ä–∏—Ç—å URL
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ Ollama
curl http://localhost:11434/api/tags

# –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
```

**–†–µ—à–µ–Ω–∏–µ 3**: –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Ollama
```bash
# –£–¥–∞–ª–∏—Ç—å Ollama –∏ –ø–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å ollama.ai
```

---

### ‚ùå "Out of memory"

**–†–µ—à–µ–Ω–∏–µ 1**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å quantized –º–æ–¥–µ–ª—å
```bash
ollama pull gemma:2b-q4_K_M  # –ú–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
```

**–†–µ—à–µ–Ω–∏–µ 2**: –ó–∞–∫—Ä—ã—Ç—å –¥—Ä—É–≥–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
- –ë—Ä–∞—É–∑–µ—Ä—ã
- IDE
- –ú–µ–¥–∏–∞–ø–ª–µ–µ—Ä—ã

**–†–µ—à–µ–Ω–∏–µ 3**: –†–∞—Å—à–∏—Ä–∏—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é –ø–∞–º—è—Ç—å (Windows)
```powershell
# Settings ‚Üí Advanced System Settings ‚Üí 
# Performance ‚Üí Advanced ‚Üí Virtual Memory
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –Ω–∞ 8-16 –ì–ë –Ω–∞ SSD
```

---

### ‚ùå "Slow responses" (–º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã)

**–†–µ—à–µ–Ω–∏–µ 1**: –£–º–µ–Ω—å—à–∏—Ç—å max_tokens
```json
{
    "llm": {
        "max_tokens": 256  // –í–º–µ—Å—Ç–æ 2048
    }
}
```

**–†–µ—à–µ–Ω–∏–µ 2**: –í–∫–ª—é—á–∏—Ç—å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ
- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å NVIDIA CUDA 11.8+
- Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU

**–†–µ—à–µ–Ω–∏–µ 3**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Phi –≤–º–µ—Å—Ç–æ Gemma (–µ—â—ë –±—ã—Å—Ç—Ä–µ–µ)
```bash
ollama pull phi:2b
```

---

### ‚ùå "–ü–ª–æ—Ö–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"

**–†–µ—à–µ–Ω–∏–µ 1**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å system_prompt
```python
response = gemma_provider.generate_response(
    prompt=prompt,
    system_prompt="–¢—ã - –æ–ø—ã—Ç–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
    temperature=0.7
)
```

**–†–µ—à–µ–Ω–∏–µ 2**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
```bash
ollama pull mistral:7b  # –õ—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ
```

**–†–µ—à–µ–Ω–∏–µ 3**: Fine-tune –º–æ–¥–µ–ª—å (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ)
- –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
- –¢—Ä–µ–±—É–µ—Ç GPU
- –°–º–æ—Ç—Ä–∏—Ç–µ: https://ollama.ai/docs

---

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

| –ú–æ–¥–µ–ª—å | –†–∞–∑–º–µ—Ä | –°–∫–æ—Ä–æ—Å—Ç—å | –ö–∞—á–µ—Å—Ç–≤–æ | –†—É—Å—Å–∫–∏–π | RAM | GPU |
|--------|--------|----------|----------|---------|-----|-----|
| **Gemma 2b** ‚≠ê | 5 –ì–ë | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úì | 4 –ì–ë | ‚ùå |
| **Phi 2b** | 4 –ì–ë | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚úì | 4 –ì–ë | ‚ùå |
| **Mistral 7b** | 13 –ì–ë | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úì‚úì | 8 –ì–ë | ‚úì |
| **Llama 2 7b** | 14 –ì–ë | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úì | 8 –ì–ë | ‚úì |
| **Zephyr 7b** | 15 –ì–ë | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úì | 8 –ì–ë | ‚úì |

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: 
- 4 –ì–ë RAM ‚Üí **Gemma 2b** –∏–ª–∏ **Phi 2b**
- 8 –ì–ë RAM ‚Üí **Mistral 7b** –∏–ª–∏ **Zephyr 7b** (—Ä—É—Å—Å–∫–∏–π –ª—É—á—à–µ)
- 16+ –ì–ë RAM ‚Üí **Llama 2 70b** (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ)

---

## üîó –°—Å—ã–ª–∫–∏

- **Ollama**: https://ollama.ai
- **Gemma –Ω–∞ Hugging Face**: https://huggingface.co/google/gemma-2b
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Ollama**: https://github.com/ollama/ollama
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è Transformers**: https://huggingface.co/docs/transformers

---

## üìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: Chat-–±–æ—Ç

```python
from utils.providers.llm.gemma_provider import GemmaLLMProvider

provider = GemmaLLMProvider(config)
provider.initialize()

# –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
messages = [
    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç, Gemma!"},
]

prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages])
prompt += "\nassistant: "

response = provider.generate_response(
    prompt=prompt,
    system_prompt="–¢—ã - –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.",
    temperature=0.7,
    max_tokens=256
)

print(f"Gemma: {response}")
```

### –ü—Ä–∏–º–µ—Ä 2: –° –ø–æ—Ç–æ–∫–æ–º

```python
print("Gemma: ", end="", flush=True)

for chunk in provider.stream_response(
    prompt="–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –æ Python",
    temperature=0.5,
    max_tokens=512
):
    print(chunk, end="", flush=True)

print()  # –ù–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –∫–æ–Ω—Ü–µ
```

### –ü—Ä–∏–º–µ—Ä 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Arvis

```python
from src.core.arvis_core import ArvisCore

# –í ArvisCore.__init__
def init_components():
    # ...
    gemma_provider = GemmaLLMProvider(self.config)
    self.operation_mode_manager.register_provider(gemma_provider)
    
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å fallback
    response = self.operation_mode_manager.llm_fallback.execute(
        lambda p: p.generate_response(prompt),
        operation_name="llm_response"
    )
```

---

## ‚úÖ Checklist —É—Å—Ç–∞–Ω–æ–≤–∫–∏

- [ ] Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ (`ollama --version`)
- [ ] Gemma 2b –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (`ollama list | grep gemma`)
- [ ] Gemma –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è (`ollama run gemma:2b` ‚Üí –æ—Ç–≤–µ—Ç–∏—Ç –Ω–∞ —Ç–µ—Å—Ç)
- [ ] Config.json –æ–±–Ω–æ–≤–ª–µ–Ω —Å `gemma:2b`
- [ ] GemmaLLMProvider —Å–æ–∑–¥–∞–Ω –≤ `utils/providers/llm/`
- [ ] –¢–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç (`pytest tests/test_gemma_provider.py`)
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ –≤ ArvisCore

---

**–í–µ—Ä—Å–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞**: 1.0  
**–°—Ç–∞—Ç—É—Å**: –ì–æ—Ç–æ–≤–æ  
**–î–∞—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è**: October 21, 2025
