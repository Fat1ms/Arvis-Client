"""
Gemma 2B LLM Provider (Ollama –∏–ª–∏ –ø—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
Gemma 2B LLM –ü—Ä–æ–≤–∞–π–¥–µ—Ä —á–µ—Ä–µ–∑ Ollama –∏–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã

Priority: 2 (–≤—ã—Å–æ–∫–∏–π, –Ω–æ –Ω–∏–∂–µ Ollama)
–Ø–∑—ã–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: –†—É—Å—Å–∫–∏–π + English
"""

from typing import Optional, Generator
import json

from config.config import Config
from utils.logger import ModuleLogger
from utils.providers import LLMProvider, ProviderStatus


class GemmaLLMProvider(LLMProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ Gemma 2B.
    
    –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
    1. Ollama (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è) - —Ç—Ä–µ–±—É–µ—Ç `ollama serve`
    2. Direct (–ø—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è) - —Ç—Ä–µ–±—É–µ—Ç `transformers`, `torch`
    
    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - model_id: ID –º–æ–¥–µ–ª–∏ ("gemma:2b" –¥–ª—è Ollama –∏–ª–∏ "google/gemma-2b-it" –¥–ª—è transformers)
    - mode: "ollama" –∏–ª–∏ "direct"
    - quantization: –¢–∏–ø –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è Ollama ("Q4_K_M", "Q5_K_M", "Q8_0", None)
    
    –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - Ollama: localhost:11434 –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω
    - Direct: torch, transformers, bitsandbytes (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """

    def __init__(self, config: Config, mode: str = "ollama", quantization: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemma 2B –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Args:
            config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            mode: "ollama" (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –∏–ª–∏ "direct" (–ø—Ä—è–º–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
            quantization: –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è Ollama (–Ω–∞–ø—Ä–∏–º–µ—Ä "Q4_K_M")
        """
        super().__init__("gemma-2b")
        self.config = config
        self.mode = mode
        self.quantization = quantization
        self.model_instance = None
        self.tokenizer = None
        self._initialized = False
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.model_id = config.get("llm.gemma_model_id", "gemma:2b")
        self.ollama_url = config.get("llm.ollama_url", "http://localhost:11434")
        self.temperature = config.get("llm.temperature", 0.7)
        self.max_tokens = config.get("llm.max_tokens", 512)
        
        self.logger.info(f"üéØ Gemma 2B Provider initialized (mode={mode}, quantization={quantization})")

    def get_priority(self) -> int:
        """
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        Gemma 2B - –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç.
        """
        return 2  # –ù–∏–∂–µ Ollama (1), –Ω–æ –≤—ã—à–µ –æ–±–ª–∞—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ (20+)

    def initialize(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Gemma 2B –ø—Ä–æ–≤–∞–π–¥–µ—Ä.
        –í—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞, –∏–Ω–∞—á–µ False
        """
        try:
            if self._initialized:
                return self.is_available()

            self.logger.info(f"üöÄ Initializing Gemma 2B ({self.mode} mode)...")

            if self.mode == "ollama":
                success = self._init_ollama()
            elif self.mode == "direct":
                success = self._init_direct()
            else:
                raise ValueError(f"Unknown mode: {self.mode}")

            if success:
                self._initialized = True
                self._status.value = "available"
                self.logger.info("‚úÖ Gemma 2B initialized successfully")
                return True
            else:
                self._status.value = "unavailable"
                return False

        except Exception as e:
            self.logger.error(f"‚ùå Failed to initialize Gemma 2B: {e}")
            self.set_error(str(e))
            self._status.value = "error"
            return False

    def _init_ollama(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Gemma 2B —á–µ—Ä–µ–∑ Ollama.
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - Ollama —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ –∑–∞–ø—É—â–µ–Ω–∞ –Ω–∞ localhost:11434
        - –ú–æ–¥–µ–ª—å gemma:2b –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
        """
        try:
            import requests
            
            # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä–∞
            health_url = f"{self.ollama_url}/api/tags"
            self.logger.info(f"üîç Checking Ollama at {health_url}...")
            
            try:
                response = requests.get(health_url, timeout=5)
                if response.status_code != 200:
                    self.logger.error(f"Ollama returned status {response.status_code}")
                    return False
                    
                # –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
                models_data = response.json()
                models = [m.get("name", "") for m in models_data.get("models", [])]
                
                if not models:
                    self.logger.warning("No models found in Ollama. Please run: ollama pull gemma:2b")
                    return False
                
                self.logger.info(f"üì¶ Available models: {', '.join(models)}")
                
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ Gemma
                gemma_found = any("gemma" in m for m in models)
                if not gemma_found:
                    self.logger.error("Gemma model not found. Please run: ollama pull gemma:2b")
                    return False
                
                self.logger.info("‚úÖ Ollama health check passed")
                self.model_instance = "ollama"  # –§–ª–∞–≥ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama
                return True
                
            except requests.exceptions.ConnectionError:
                self.logger.error(f"Cannot connect to Ollama at {self.ollama_url}")
                self.logger.info("üí° Please run: ollama serve")
                return False
                
        except ImportError:
            self.logger.error("requests library not found. Install it: pip install requests")
            return False
        except Exception as e:
            self.logger.error(f"Ollama initialization error: {e}")
            return False

    def _init_direct(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Gemma 2B –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π (transformers).
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        - torch —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        - transformers —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        - ~6 –ì–ë —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏
        
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
        """
        try:
            self.logger.info("üì• Loading Gemma 2B model with transformers...")
            
            try:
                from transformers import AutoTokenizer, AutoModelForCausalLM
                import torch
            except ImportError:
                self.logger.error("transformers or torch not found. Install: pip install transformers torch")
                return False
            
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–≥—Ä—É–∑–∫–∏
            model_name = "google/gemma-2b-it"  # '-it' = Instruct-tuned –≤–µ—Ä—Å–∏—è
            
            self.logger.info(f"üîÑ Loading tokenizer from {model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            self.logger.info(f"üîÑ Loading model (this may take a minute)...")
            self.model_instance = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,  # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å
                device_map="auto",           # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞ GPU –µ—Å–ª–∏ –µ—Å—Ç—å
                low_cpu_mem_usage=True,      # –≠–∫–æ–Ω–æ–º–∏—Ç –ø–µ—Ä–≤–∏—á–Ω—É—é –ø–∞–º—è—Ç—å
            )
            
            # –í–∫–ª—é—á–∏—Ç—å eval —Ä–µ–∂–∏–º
            self.model_instance.eval()
            
            self.logger.info("‚úÖ Gemma 2B model loaded successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Direct initialization failed: {e}")
            return False

    def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Gemma 2B –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Returns:
            True –µ—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ, –∏–Ω–∞—á–µ False
        """
        try:
            if not self._initialized or self.model_instance is None:
                return False

            if self.mode == "ollama":
                # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Ollama —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
                import requests
                try:
                    response = requests.get(f"{self.ollama_url}/api/tags", timeout=2)
                    return response.status_code == 200
                except:
                    return False
            else:
                # Direct mode - –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –æ–Ω–∞ –¥–æ—Å—Ç—É–ø–Ω–∞
                return self.model_instance is not None

        except Exception as e:
            self.logger.debug(f"Gemma availability check failed: {e}")
            return False

    def generate_response(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å.
        
        –§–æ—Ä–º–∏—Ä—É–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤—ã–∑—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.0 - 1.0), –µ—Å–ª–∏ None –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ config
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤, –µ—Å–ª–∏ None –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ config
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ä–æ–ª–∏ –º–æ–¥–µ–ª–∏
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            
        Raises:
            RuntimeError: –ï—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        """
        if not self.is_available():
            raise RuntimeError("Gemma 2B is not available")

        try:
            temp = temperature if temperature is not None else self.temperature
            tokens = max_tokens if max_tokens is not None else self.max_tokens
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            full_prompt = self._prepare_prompt(prompt, system_prompt)
            
            if self.mode == "ollama":
                return self._generate_ollama(full_prompt, temp, tokens)
            else:
                return self._generate_direct(full_prompt, temp, tokens)

        except Exception as e:
            self.logger.error(f"‚ùå Generation failed: {e}")
            raise RuntimeError(f"Gemma generation error: {e}")

    def stream_response(
        self,
        prompt: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None,
    ) -> Generator[str, None, None]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ—Ç–æ–∫–æ–º (streaming).
        
        –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∞—Ç—å –æ—Ç–≤–µ—Ç –ø–æ —á–∞—Å—Ç—è–º –ø–æ –º–µ—Ä–µ –µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0.0 - 1.0)
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            
        Yields:
            –ß–∞–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞
            
        Raises:
            RuntimeError: –ï—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        """
        if not self.is_available():
            raise RuntimeError("Gemma 2B is not available")

        try:
            temp = temperature if temperature is not None else self.temperature
            tokens = max_tokens if max_tokens is not None else self.max_tokens
            
            full_prompt = self._prepare_prompt(prompt, system_prompt)
            
            if self.mode == "ollama":
                yield from self._stream_ollama(full_prompt, temp, tokens)
            else:
                yield from self._stream_direct(full_prompt, temp, tokens)

        except Exception as e:
            self.logger.error(f"‚ùå Stream generation failed: {e}")
            raise RuntimeError(f"Gemma stream error: {e}")

    def _prepare_prompt(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–æ–º–ø—Ç —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.
        
        –§–æ—Ä–º–∞—Ç –¥–ª—è Gemma (Instruct-tuned):
        ```
        <|im_start|>system
        {system_prompt}
        <|im_end|>
        <|im_start|>user
        {prompt}
        <|im_end|>
        <|im_start|>assistant
        ```
        
        Args:
            prompt: –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        if system_prompt:
            full = f"<|im_start|>system\n{system_prompt}\n<|im_end|>\n"
        else:
            full = ""
        
        full += f"<|im_start|>user\n{prompt}\n<|im_end|>\n"
        full += "<|im_start|>assistant\n"
        
        return full

    def _generate_ollama(self, prompt: str, temperature: float, max_tokens: int) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ Ollama API.
        
        Args:
            prompt: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        import requests
        
        url = f"{self.ollama_url}/api/generate"
        
        payload = {
            "model": self.model_id,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
                "top_p": 0.95,
                "top_k": 40,
            }
        }
        
        try:
            response = requests.post(url, json=payload, timeout=120)
            response.raise_for_status()
            
            data = response.json()
            return data.get("response", "").strip()
            
        except requests.exceptions.Timeout:
            self.logger.error("Ollama generation timeout (120s exceeded)")
            raise RuntimeError("Generation timeout")
        except Exception as e:
            self.logger.error(f"Ollama generation error: {e}")
            raise

    def _stream_ollama(self, prompt: str, temperature: float, max_tokens: int) -> Generator[str, None, None]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–º —á–µ—Ä–µ–∑ Ollama API.
        
        Args:
            prompt: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤
            
        Yields:
            –ß–∞–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞
        """
        import requests
        
        url = f"{self.ollama_url}/api/generate"
        
        payload = {
            "model": self.model_id,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens,
            }
        }
        
        try:
            response = requests.post(url, json=payload, stream=True, timeout=120)
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    chunk = data.get("response", "")
                    if chunk:
                        yield chunk
                    
                    # –ï—Å–ª–∏ done=true, –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ–º
                    if data.get("done", False):
                        break
                        
        except Exception as e:
            self.logger.error(f"Ollama streaming error: {e}")
            raise

    def _generate_direct(self, prompt: str, temperature: float, max_tokens: int) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π (transformers).
        
        Args:
            prompt: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        try:
            import torch
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            # –ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
            device = next(self.model_instance.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å
            with torch.no_grad():
                output_ids = self.model_instance.generate(
                    **inputs,
                    max_length=len(inputs["input_ids"][0]) + max_tokens,
                    temperature=temperature,
                    top_p=0.95,
                    do_sample=True,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.pad_token_id,
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            
            # –û—Ç—Ä–µ–∑–∞—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if "<|im_start|>assistant" in response:
                response = response.split("<|im_start|>assistant")[-1]
            if "<|im_end|>" in response:
                response = response.split("<|im_end|>")[0]
            
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"Direct generation error: {e}")
            raise

    def _stream_direct(self, prompt: str, temperature: float, max_tokens: int) -> Generator[str, None, None]:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–º –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π.
        
        –°–∏–º—É–ª–∏—Ä—É–µ—Ç streaming –ø—É—Ç—ë–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —á–∞—Å—Ç—è–º.
        
        Args:
            prompt: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            max_tokens: –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤
            
        Yields:
            –ß–∞–Ω–∫–∏ –æ—Ç–≤–µ—Ç–∞
        """
        try:
            # –î–ª—è –ø—Ä—è–º–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –Ω–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ streaming
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–µ–ª–∏–∫–æ–º –∏ –≤—ã–¥–∞—ë–º –ø–æ —Å–ª–æ–≤–∞–º
            response = self._generate_direct(prompt, temperature, max_tokens)
            
            # –í—ã–¥–∞–≤–∞—Ç—å –ø–æ —Å–ª–æ–≤–∞–º
            words = response.split()
            for i, word in enumerate(words):
                chunk = word + (" " if i < len(words) - 1 else "")
                yield chunk
                
        except Exception as e:
            self.logger.error(f"Direct streaming error: {e}")
            raise

    def shutdown(self) -> bool:
        """
        –ó–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É Gemma 2B.
        
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
        """
        try:
            if self.mode == "direct":
                # –û—Å–≤–æ–±–æ–¥–∏—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏
                import torch
                if self.model_instance is not None:
                    del self.model_instance
                    torch.cuda.empty_cache()
            
            self.model_instance = None
            self.tokenizer = None
            self._initialized = False
            self._status.value = "unavailable"
            
            self.logger.info("‚úÖ Gemma 2B shutdown complete")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Gemma 2B shutdown failed: {e}")
            return False

    def get_model_info(self) -> dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
        """
        return {
            "name": "Gemma 2B",
            "provider": "google",
            "parameters": 2_000_000_000,
            "context_length": 8192,
            "mode": self.mode,
            "quantization": self.quantization,
            "available": self.is_available(),
            "priority": self.get_priority(),
            "language_support": ["english", "russian", "multilingual"],
        }
